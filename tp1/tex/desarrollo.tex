\subsection{Detalle de los algoritmos utilizados}
\subsubsection{Eliminación Gaussiana}
La Eliminación Gaussiana es un método que consiste en resolver el sistema triangulando la matriz (aplicando operaciones entre filas) y aplicando las mismas operaciones al vector solución, para obtener un sistema equivalente pero con una matriz triangular superior que se puede resolver de manera sencilla mediante \emph{backward substitution}.

El algoritmo de trabaja con la matriz ampliada del sistema:

\[\left(\begin{array}{ccc|c}
a_{11} & \ldots & a_{1n} & b_1\\
\vdots & \ddots & \vdots & \vdots\\
a_{n1} & \ldots & a_{nn} & b_n
\end{array}\right)\]

En cada paso $k$ el algoritmo produce, mediante operaciones de filas, la aparición de ceros debajo de la diagonal en la columna $k$:

\begin{algorithmic}
 \For{$k \gets 1 \ldots n$}
  \For{$i \gets k+1 \ldots n$}
   \For{$j \gets k \ldots n$}
    \State $A[i, j] \gets A[i, j] - A[k, j]*(A[i, j]/A[k, k])$
   \EndFor
  \EndFor
 \EndFor
\end{algorithmic}

Notemos que es necesario pedir que ningún valor de la diagonal se anule en ningún paso del algoritmo. Más adelante veremos que esto se puede garantizar gracias a la estructura de nuestra matriz.

El algoritmo tiene complejidad $\mathcal{O}(n^3)$ para una matriz $A \in \mathbb{R}^{n \times n}$ ya que realiza $\mathcal{O}(n)$ operaciones escalares por fila por paso; hay $n$ filas y realiza $n-1$ pasos.

\subsubsection{Backward substitution}
Dada una matriz triangular superior $A$ sin elementos nulos en la diagonal, entonces el vector solución $x$ es único y se obtiene haciendo

\begin{algorithmic}
 \For{$i \gets n \ldots 1$}
  \State $x[i] \gets b_n$
  \For{$j \gets i+1 \ldots n$}
   \State $x[i] \gets x[i] - A[i,j]*x[j]$
  \EndFor
  \State $x[i] \gets x[i] / A[i, i]$
 \EndFor
\end{algorithmic}

Notemos nuevamente la importancia de que los elementos de la diagonal sean no nulos para que las operaciones estén bien definidas.

Este algoritmo tiene complejidad $\mathcal{O}(n^2)$ pues para despejar cada incógnita realiza $\mathcal{O}(n)$ operaciones y hay $n$ incógnitas.

\subsubsection{Forward substitution}
La idea es idéntica a backward substitution para matrices triangulares inferiores, invirtiendo el orden en que se despejan las incógnitas.

\begin{algorithmic}
 \For{$i \gets 1 \ldots n$}
  \State $x[i] \gets b_n$
  \For{$j \gets 1 \ldots i-1$}
   \State $x[i] \gets x[i] - A[i,j]*x[j]$
  \EndFor
  \State $x[i] \gets x[i] / A[i, i]$
 \EndFor
\end{algorithmic}

\subsubsection{Factorización LU}

El método de Factorización LU consiste en aplicar el método de Eliminación Gaussiana pero ``almacenando'' las operaciones realizadas para obtener una factorización $A = LU$ con $L$ triangular inferior (con unos en la diagonal) y $U$ triangular superior. Más formalmente, cada paso $k$ de la Eliminación Gaussiana es equivalente a multiplicar a la matriz $A^{(k-1)}$ a izquierda por la matriz

\[
M_k = 
\begin{pmatrix} 
1 		& \ldots 	& 0 				& \ldots 	& 0 \\
0 		& \ddots 	& 0 				& \ldots 	& 0 \\
\vdots 	& 			& 1 				& 			& \vdots\\
0		& \ldots		& -\frac{a_{(k+1)k}^{(k - 1)}}{a_{kk}^{(k - 1)}} 	& \ldots		& 0\\
\vdots	& 			& \vdots		 	& \ddots		& \vdots\\
0		& \ldots		& -\frac{a_{(n)k}^{(k - 1)}}{a_{kk}^{(k - 1)}}	& \ldots		& 1\\
\end{pmatrix}
\]

por lo que el resultado de la Eliminación Gaussiana es una matriz triangular superior $U$ que coincide con $M_{n-1} \ldots M_1 A$. Pero como todas las matrices $M_k$ son inversibles por ser triangulares inferiores con unos en la diagonal, vale $A = M_1^{-1}\ldots M_{n - 1}^{-1} U = LU$.
 
Si bien ya hemos llegado a la factorización LU de $A$, podemos simplificar la expresión de $L$. Notemos que $M_k = I_n - m_k e_k^t$ con $m_k = \begin{pmatrix}0\\ \vdots \\ 0 \\ m_{(k+1)k} \\ \vdots \\ m_{nk}\end{pmatrix}$ y $e_k$ el $k$-ésimo vector canónico de $\mathbb{R}^n$. Usando esta escritura es fácil ver que $M_k^{-1} = I_n + m_k e_k^t$, ya que

\begin{align*}
(I_n - m_k e_k^t)(I_n + m_k e_k^t) & = I_n^2 + m_k e_k^t - m_k e_k^t - m_k e_k^t m_k e_k^t &\\
& = I_n^2 - m_k e_k^t m_k e_k^t &\\
& = I_n & \text{(pues } e_k^t m_k = 0\text{)}
\end{align*}

Luego,

\begin{align*}
L & = M_1^{-1} \cdots M_{n - 1}^{-1} &\\
    & = (I_n + m_1 e_1^t) \cdots (I_n + m_{n - 1} e_{n - 1}^t) &\\
    & = I_n + m_1 e_1^t + \cdots + m_{n - 1} e_{n - 1}^t & \text{(pues } e_i^t m_j = 0 \text{ si } i \leq j \text{)}
\end{align*}

En definitiva $A = LU$ con $L = \begin{pmatrix} 
1           & \cdots    & 0                 & \cdots    & 0 \\
m_{21}      & \ddots    & 0                 & \cdots    & 0 \\
\vdots      &           & 1                 &           & \vdots\\
m_{(k+1)1}  & \cdots        & m_{(k+1)k}    & \cdots        & 0\\
\vdots      &           & \vdots            & \ddots        & \vdots\\
m_{n1}      & \cdots        & m_{nk}        & \cdots        & 1\\
\end{pmatrix}$.

Notemos que es posible ir armando $L$ a cada paso de la Eliminación Gaussiana por lo que la complejidad computacional se mantiene: computar la factorización $LU$ toma también $\mathcal{O}(n^3)$.

Luego, dado cualquier vector $b$, para resolver el sistema $Ax=b$ alcanza con calcular $y$ tal que $Ly=b$ y luego $x$ tal que $Ux=y$. El primer sistema se puede resolver en $\mathcal{O}(n^2)$ por ser $L$ triangular inferior (usamos \emph{forward substitution}, que es igual que \emph{backward} pero a la inversa) y el segundo también, por ser $U$ triangular superior (usamos \emph{backward substitution} normalmente).

De este modo vemos que, si para una misma matriz $A$ y mútliples vectores $b_1, \ldots, b_k$ queremos resolver todos los sistemas $Ax=b_i$, en lugar de aplicar $k$ veces la Eliminación Gaussiana (que toma $\mathcal{O}(n^3)$) parecería conveniente calcular una única vez la factorización LU en $\mathcal{O}(n^3)$ y luego usarla para resolver los $k$ sistemas en $\mathcal{O}(n^2)$. Este trabajo práctico tiene como objetivo principal contrastar dicha conjetura experimentalmente.

\subsection{Discretización}
Para discretizar el problema limitaremos la cantidad de radios y la cantidad de ángulos a evaluar, y una vez elegidas dichas cantidades los distribuiremos de manera uniforme por la corona circular. Así, los puntos de la discretización serán los puntos $P_{j,k}$ de radio $r_j$ y ángulo $\theta_k$ con $j = 0, \ldots, m$; $k = 0, \ldots, n$ que cumplan $$r_0 = r_i$$ $$r_m = r_e$$ $$\theta_0 = 0$$ $$\theta_n = 2\pi$$ $$r_{j+1}-r_{j} = \Delta r \text{ constante } \forall j = 0, \ldots, m-1$$ $$\theta_{k+1}-\theta_{k} = \Delta \theta \text{ constante } \forall \theta = 0, \ldots, n-1$$

Llamaremos $t_{j,k}$ a la temperatura del punto $P_{j,k}$ una vez alcanzado el equilibrio térmico.

De la discretización elegida y de la ecuación \ref{calor-continuo} resulta una nueva ecuación que nos permitirá armar nuestro sistema de ecuaciones\footnote{Los detalles sobre la discretización de la ecuación pueden encontrarse en el Apéndice \ref{enunciado}}.

\begin{equation}\label{calor}
\frac{t_{j-1,k}-2t_{jk}+t_{j+1,k}}{(\Delta r)^2}+\frac{1}{r_j}\frac{t_{j,k}-t_{j-1,k}}{\Delta r}+\frac{1}{{r_j}^2}\frac{t_{j,k-1}-2t_{jk}+t_{j,k+1}}{(\Delta \theta)^2} = 0 
\end{equation}

Esta ecuación vale para cada punto $P_{j, k}$ del modelo salvo los límites, sobre los cuales hablaremos en breve.

\subsection{Armado del sistema de ecuaciones}
Para poder armar el sistema $Ax=b$ es necesario:
\begin{itemize}
 \item
    Extraer los factores que multiplican a cada una de las cinco incógnitas en la ecuación \ref{calor}: $t_{j-1,k}$; $t_{j,k}$; $t_{j+1,k}$; $t_{j,k-1}$ y $t_{j,k+1}$
    \begin{align*}
        t_{j-1, k}&*\left(\frac{1}{(\Delta r)^2} - \frac{1}{r_j \Delta r}\right) \\
        t_{j, k}  &*\left(\frac{-2}{(\Delta r)^2} + \frac{1}{r_j \Delta r} - \frac{2}{{r_j}^2 (\Delta \theta)^2}\right) \\
        t_{j+1, k}&*\left(\frac{1}{(\Delta r)^2}\right) \\
        t_{j, k-1}&*\left(\frac{1}{{r_j}^2(\Delta \theta)^2}\right) \\
        t_{j, k+1}&*\left(\frac{1}{{r_j}^2(\Delta \theta)^2}\right)
    \end{align*}

 \item
    Analizar los ``casos borde'': aquellos puntos donde la ecuación \ref{calor} no vale.
    
    Para evitar confusiones, tomaremos $\theta_0 = 0$ como el menor valor posible de $\theta$ y $\theta_{n-1}$ como el mayor, pues vale $P_{j, n} = P_{j, 0}$ para cualquier $j$. 
    
    Los casos interesantes para valores de $j, k$ entonces son:
    \begin{enumerate}
     \item La pared interior del horno ($j = 0$; $k = 0, \ldots, n-1$). La ecuación en esos casos es $$t_{0, k} = T_i(\theta_k)$$
     \item La pared exterior del horno ($j = m$; $k = 0, \ldots, n-1$). La ecuación en esos casos es $$t_{m, k} = T_e(\theta_k)$$
     \item El valor mínimo de $\theta$ ($j = 0, \ldots, m$; $k = 0$). Se debe reemplazar $t_{j, k-1}$ por $t_{j, n-1}$ en todas las ecuaciones correspondientes.
     \item El valor máximo de $\theta$ ($j = 0, \ldots, m$; $k = n-1$). Se debe reemplazar $t_{j, k+1}$ por $t_{j, 0}$ en todas las ecuaciones correspondientes.
    \end{enumerate}
    Estos últimos dos reemplazos se pueden resumir en $$(j, k) \Rightarrow (j, k \text{ mod } n)$$
 \item
    Combinar los puntos anteriores para plantear el sistema de ecuaciones a resolver:
    \begin{align*}\label{sistema}
    &t_{0, k} = T_i(\theta_k)                                           &\forall k = 0, \ldots, n-1  \\
    &t_{m, k} = T_e(\theta_k)                                           &\forall k = 0, \ldots, n-1  \\
    &   t_{j-1, k}\left(\frac{1}{(\Delta r)^2} - \frac{1}{r_j \Delta r}\right) + 
        t_{j, k}  \left(\frac{-2}{(\Delta r)^2} + \frac{1}{r_j \Delta r} - \frac{2}{{r_j}^2 (\Delta \theta)^2}\right) + \\
    &   t_{j+1, k}\left(\frac{1}{(\Delta r)^2}\right) + 
        t_{j, k-1}\left(\frac{1}{{r_j}^2(\Delta \theta)^2}\right) + 
        t_{j, k+1}\left(\frac{1}{{r_j}^2(\Delta \theta)^2}\right) = 0  &\forall j=1, \ldots, m-1; k = 1, \ldots , n-2 \\
    \end{align*}
    Y, para $k = 0$, $k = n-1$, nuevamente la última ecuación pero tomando $\text{mod } n$ luego de sumarle o restarle $1$ a $k$.
    
    Del sistema podemos obtener la matriz $A$ (que tendrá 5 valores no nulos por fila a lo sumo) y el vector $b$ (que será nulo en todas sus componentes salvo aquellas correspondientes a $j=0$ y $j=m$).
  \item
    Pensar un orden para las incógnitas que permita asegurar que la matriz resultante sea $banda$. La importancia de esta propiedad se tratará en breve. El mismo es:
    
    $$ t_{0,0}; t_{0,1}; \ldots ; t_{0,n-1}; t_{1,0}; t_{1,1}; \ldots ; t_{j,n-1}; t_{j+1,0}; t_{j+1,1}; \ldots ; t_{m, n-2} ; t_{m, n-1}$$ % TODO: estaría bueno hacer una imagen representativa, que muestre un toque la espiral rara esta. No sé usar mucho ninguna herramienta como para hacerlo :(.
    
    tanto para las filas como para las columnas. Este orden garantiza que la distancia máxima de un punto $P_{j,k}$ hasta sus vecinos en la matriz es de $n$ posiciones (para los casos $P_{j+1, k}$ y $P_{j-1, k}$) y por lo tanto el ``ancho'' de la banda es de $2n$ (salvo para las filas que coinciden con la identidad, cuyo ancho es 1).
    
\end{itemize}

Una vez realizados estos pasos estamos en condiciones de plantear el sistema de ecuaciones $Ax=b$:

Lo primero que debemos notar es que como hay $n*(m+1)$ puntos diferentes tendremos $n*(m+1)$ incógnitas diferentes. Luego, $A \in \mathbb{R}^{n(m+1)\times n(m+1)}$: cada columna y cada fila de $A$ corresponden a un punto $P_{j,k}$ del sistema. Asimismo, $x \in \mathbb{R}^{n(m+1)}$ y $b \in \mathbb{R}^{n(m+1)}$. 

Lo segundo que debemos notar es que, por coincidir el orden elegido para filas y para columnas, el índice de la fila correspondiente al punto $P_{j,k}$ coincide con el de la columna correspondiente a ese punto. Llamaremos a este índice $i(j,k)$. Notar que podemos computar $i$ fácilmente como $i(j,k)=j*n+k$ (suponiendo que indexamos por 0 tanto filas como columnas).

Por el orden elegido, las primeras $n$ filas corresponden a los puntos $P_{0,k}$ con $k=0,\ldots,n-1$. Mirando el sistema de ecuaciones, las primeras $n$ filas de $A$ coinciden con la identidad (1 en la diagonal y 0 en el resto) y las primeras $n$ filas de $b$ coinciden con $T_i(\theta_k)$.

Lo mismo vale para las últimas $n$ filas: corresponden a los puntos $P_{m,k}$ con $k=0,\ldots,n-1$, las filas correspondientes de $A$ coinciden con la identidad y las componentes de $b$ con $T_e(\theta_k)$.

Llegado este punto podemos definir completamente $b$: todas sus demás componentes son nulas (por ser $0$ la solución al resto de las ecuaciones del sistema), por lo que resulta:

$$b = (T_i(0), T_i(1), \ldots, T_i(n-1), 0, \ldots, 0, T_e(0), T_e(1), \ldots, T_e(n-1)) $$

Para $j \not = 0, j \not = m$, las filas $i(j,k)$ de $A$ tendrán cinco componentes no-nulas (que corresponden a los vecinos de $P_{j,k}$ en la discretización). Fijados $j$ y $k$ ($0\not=j\not=m, 0\not=k\not=n-1$), estas componentes serán $i(j-1,k); i(j,k); i(j+1,k); i(j,k-1)$ e $i(j,k+1)$ y coincidirán con lo que en el sistema de ecuaciones aparece multiplicando a cada una de esas incógnitas.

Resta simplemente considerar los casos $k=0$ y $k=n-1$, pero no revisten mayor complejidad que tomar módulo $n$ después de las operaciones que involucren $k$.

\subsection{Resolución del sistema de ecuaciones}
\subsubsection{Caracterización de la matriz}

\begin{proposition}

La matriz $A$ es banda y diagonal dominante (no estricta) por filas.

\begin{proof}

El hecho de que es banda fue fundamentado al momento de construirla.

Resta ver que es diagonal dominante (no estricta) por filas, es decir, que $$|a_{i, i}| \geq \sum\limits_{j \neq i}^{n*(m+1)} |a_{i, j}| \hspace{10pt} \forall i = 1, \ldots, n*(m+1)$$

Las primeras y últimas $n$ filas de $A$ coinciden con la identidad, por lo que esto vale trivialmente. Dada cualquier otra fila $i(j, k)$ con $j \neq 0, j \neq m$ debemos recordar que hay solo 5 valores no nulos, por lo que en realidad queremos probar que el elemento de la diagonal es mayor, en módulo, a la suma en módulo de los otros cuatro elementos.

Reemplazando por los valores de los multiplicadores, se obtiene

$$\left|\frac{-2}{(\Delta r)^2} + \frac{1}{r_j \Delta r} - \frac{2}{{r_j}^2 (\Delta \theta)^2}\right| \geq \left|\frac{1}{(\Delta r)^2} - \frac{1}{r_j \Delta r}\right| + \left|\frac{1}{(\Delta r)^2}\right| + 2 * \left|\frac{1}{{r_j}^2(\Delta \theta)^2}\right| $$

Quitando el módulo en los valores claramente positivos y reordenando, tenemos

$$\left|\frac{1}{r_j \Delta r} - \frac{2}{(\Delta r)^2} - \frac{2}{{r_j}^2 (\Delta \theta)^2}\right| \geq \left|\frac{1}{(\Delta r)^2} - \frac{1}{r_j \Delta r}\right| + \frac{1}{(\Delta r)^2} + \frac{2}{{r_j}^2(\Delta \theta)^2} $$

En este punto nos detuvimos a analizar cada caso en particular, pero pronto descubrimos que solo vale la pena uno:
como sabemos $j \neq 0$, entonces $r_j \geq \Delta r$ para cualquier valor de $j$, pues hasta el primer radio distinto al interior hay por lo menos un incremento en la discretización. Por lo tanto 
$$\frac{1}{r_j \Delta r} \leq \frac{1}{(\Delta r)^2}$$
$$0 \leq \frac{1}{(\Delta r)^2} - \frac{1}{r_j \Delta r}$$

Por lo que podemos omitir el módulo de la derecha. Asimismo, 
$$ \frac{1}{r_j \Delta r} \leq \frac{1}{(\Delta r)^2} $$
$$ \frac{1}{r_j \Delta r} < \frac{2}{(\Delta r)^2} $$
$$ \frac{1}{r_j \Delta r} - \frac{2}{(\Delta r)^2} < 0 $$

Y por ser $\frac{2}{{r_j}^2 (\Delta \theta)^2} > 0$ podemos afirmar
$$ \frac{1}{r_j \Delta r} - \frac{2}{(\Delta r)^2} - \frac{2}{{r_j}^2 (\Delta \theta)^2} < 0 $$

Por lo que podemos deshacernos del módulo de la izquierda negando sus términos. 
$$\frac{2}{(\Delta r)^2} - \frac{1}{r_j \Delta r} + \frac{2}{{r_j}^2 (\Delta \theta)^2} \geq \frac{1}{(\Delta r)^2} - \frac{1}{r_j \Delta r} + \frac{1}{(\Delta r)^2} + \frac{2}{{r_j}^2(\Delta \theta)^2} $$

$$\frac{2}{(\Delta r)^2} - \frac{1}{r_j \Delta r} + \frac{2}{{r_j}^2 (\Delta \theta)^2} \geq \frac{2}{(\Delta r)^2} - \frac{1}{r_j \Delta r} + \frac{2}{{r_j}^2(\Delta \theta)^2} $$

Con lo cual la desigualdad es trivialmente cierta (y de hecho resulta ser una igualdad, lo cual fundamenta por qué no es ``estrictamente'' diagonal dominante).
\end{proof}
\end{proposition}
\subsubsection{Resolución sin pivoteo}
Nos interesa demostrar que es posible aplicar Eliminación Gaussiana sin pivoteo en la matriz $A$.

Para su demostración usaremos los siguientes lemas:
\begin{lemma}\label{submatriz}
 Aplicar un paso de la Eliminación Gaussiana sobre una matriz diagonal dominante preserva esa propiedad en la submatriz que resta triangular.
\end{lemma}
\begin{proof}[Idea de demostración]
 Igual que la demostración vista en clase teórica de que un paso de la Eliminación Gaussiana sobre una matriz \emph{estrictamente} diagonal dominante preserva esa propiedad en la submatriz que resta triangular, pero relajando la condición de \emph{estrictamente} para pedir que simplemente preserve la propiedad de diagonal dominante (cambiar los $<$ por $\leq$).
\end{proof}

\begin{lemma}\label{ancho}
 A cada paso $k$ de la Eliminación Gaussiana la fila $F_k$ con la que trabaje tendrá un elemento no nulo en la columna $k+n$, o bien será una fila de la identidad.
\end{lemma}
\begin{proof}
 Empecemos por ver que el ancho de la banda de la matriz es estricto para las filas que no coinciden con la identidad: toda fila $F_k$ tiene siempre como último elemento no nulo el de la columna $k+n$ o bien es una fila de la identidad. Esto surge directamente de observar el orden elegido para las incógnitas y viendo que todas tienen un elemento no nulo correspondiente al vecino $j+1, k$.

 Ya vimos que en la matriz original $A$ toda fila $F_k$ o bien coincide con la identidad o bien tenía como último elemento no nulo el de la columna $k+n$. Veamos ahora que esto no se puede haber alterado en ningún paso del algoritmo: de alterarse, quiere decir que a la fila $F_k$ se le restó una fila $F_x$ en algún paso $x$, $x<k$, que tenía un elemento no nulo en la columna $k+n$. Pero por ser $x<k$, el último elemento no nulo de la fila $F_x$ debe ser $x+n < k+x$. Absurdo, que provino de suponer que se podría alterar la propiedad en algún paso del algoritmo.
\end{proof}

\begin{proposition}\label{pivoteo}
 A cada paso $k$ de la Eliminación Gaussiana la fila $k$ con la que trabaje tendrá un elemento no nulo en la columna $k$, y por lo tanto el algoritmo podrá continuar normalmente sin realizar pivoteo.
\end{proposition}
\begin{proof}
 Queremos ver que al comenzar el paso $k$ de la Eliminación Gaussiana $a_{kk}^{(k-1)} \neq 0$.

 Por el lema \ref{submatriz}, al comenzar el paso $k$ de la Eliminación Gaussiana la submatriz que aún resta triangular (llamémosla $B$) es diagonal dominante. $B$ incluye parte de la fila $F_k$, en particular a partir de la columna $k$ en adelante, por lo que $a_{kk}$ es parte de su diagonal (en particular es su primer elemento).
 
 A su vez, por el lema \ref{ancho}, al comenzar el paso $k$ de la Eliminación Gaussiana la fila $F_k$ tiene un elemento no nulo en su columna $k+n$. Es decir, el elemento $a_{k(k+n)}$ es no nulo, y este elemento pertenece a $B$. Por lo que, por definición de diagonal dominante $|a_{kk}| \geq |a_{k(k+1)}| \neq 0 \Rightarrow a_{kk} \neq 0$ como queríamos demostrar. 
\end{proof}
